{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importando as bibliotecas, pacotes de pre-processamento NLTK e configurando o Swifter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\miniconda3\\envs\\infnet-nn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'indexer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\danie\\OneDrive\\Documents\\MIT em Machine Learning, Deep Learning e AI\\3 - Redes Neurais com TensorFlow\\Entrega Trabalho\\Produtos_Pre Processamento.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/OneDrive/Documents/MIT%20em%20Machine%20Learning%2C%20Deep%20Learning%20e%20AI/3%20-%20Redes%20Neurais%20com%20TensorFlow/Entrega%20Trabalho/Produtos_Pre%20Processamento.ipynb#W2sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcatboost\u001b[39;00m \u001b[39mimport\u001b[39;00m Pool, cv\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/OneDrive/Documents/MIT%20em%20Machine%20Learning%2C%20Deep%20Learning%20e%20AI/3%20-%20Redes%20Neurais%20com%20TensorFlow/Entrega%20Trabalho/Produtos_Pre%20Processamento.ipynb#W2sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mprettytable\u001b[39;00m \u001b[39mimport\u001b[39;00m PrettyTable\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/danie/OneDrive/Documents/MIT%20em%20Machine%20Learning%2C%20Deep%20Learning%20e%20AI/3%20-%20Redes%20Neurais%20com%20TensorFlow/Entrega%20Trabalho/Produtos_Pre%20Processamento.ipynb#W2sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspellchecker\u001b[39;00m \u001b[39mimport\u001b[39;00m SpellChecker\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/OneDrive/Documents/MIT%20em%20Machine%20Learning%2C%20Deep%20Learning%20e%20AI/3%20-%20Redes%20Neurais%20com%20TensorFlow/Entrega%20Trabalho/Produtos_Pre%20Processamento.ipynb#W2sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnotebook_tqd\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/OneDrive/Documents/MIT%20em%20Machine%20Learning%2C%20Deep%20Learning%20e%20AI/3%20-%20Redes%20Neurais%20com%20TensorFlow/Entrega%20Trabalho/Produtos_Pre%20Processamento.ipynb#W2sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n",
      "File \u001b[1;32mc:\\Users\\danie\\miniconda3\\envs\\infnet-nn\\lib\\site-packages\\spellchecker\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m  \u001b[39mspellchecker\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m Spellchecker,getInstance\n",
      "File \u001b[1;32mc:\\Users\\danie\\miniconda3\\envs\\infnet-nn\\lib\\site-packages\\spellchecker\\core.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minexactsearch\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mindexer\u001b[39;00m \u001b[39mimport\u001b[39;00m DictionaryIndex\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangdetect\u001b[39;00m \u001b[39mimport\u001b[39;00m _detect_lang\n\u001b[0;32m     29\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mSpellchecker\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgetInstance\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'indexer'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import unicodedata\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "import swifter\n",
    "import pickle\n",
    "from keras.layers import Dropout\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from itertools import chain\n",
    "\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from swifter import set_defaults\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import Pool, cv\n",
    "from prettytable import PrettyTable\n",
    "from spellchecker import SpellChecker\n",
    "import tqdm as notebook_tqd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Flatten, Dense,MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NLTK packages\n",
    "nltk.download('stopwords') #this package is used for removing stopwords\n",
    "nltk.download('wordnet') #this package is used for lemmatization\n",
    "nltk.download('punkt')  #this package is used for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurando Swifter\n",
    "set_defaults(\n",
    "    npartitions = None,\n",
    "    dask_threshold = 1,\n",
    "    scheduler = \"processes\",\n",
    "    progress_bar = True,\n",
    "    progress_bar_desc = None,\n",
    "    allow_dask_on_strings = True,\n",
    "    force_parallel = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importando a base de Dados para um dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'data/produtos_pre_tratamento.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explorando a base de Dados**\n",
    "\n",
    "A análise da distribuição das categorias dos itens revela uma visão abrangente das diferentes classificações presentes. A categorização dos produtos é essencial para a organização e gestão eficiente do inventário. No contexto atual, foram identificadas quatro principais categorias de produtos:\n",
    "\n",
    "Embalagem:\n",
    "\n",
    "Quantidade: 3.416 itens Percentual: 34,72% A categoria \"Embalagem\" representa o maior segmento, compreendendo cerca de um terço (34,72%) do total de itens. Essa categoria inclui produtos essenciais para acondicionar e proteger os produtos, desempenhando um papel crucial na logística e no transporte.\n",
    "\n",
    "EPI (Equipamentos de Proteção Individual):\n",
    "\n",
    "Quantidade: 2.673 itens Percentual: 27,16% A categoria \"EPI\" é significativa, abrangendo aproximadamente 27,16% do total de itens. Esses equipamentos são essenciais para garantir a segurança e o bem-estar dos trabalhadores em diversas atividades, promovendo um ambiente laboral mais seguro.\n",
    "\n",
    "Papelaria:\n",
    "\n",
    "Quantidade: 2.364 itens Percentual: 24,02% A categoria \"Papelaria\" representa cerca de 24,02% do total de itens. Itens de papelaria são essenciais para as operações administrativas e organizacionais, sendo elementos básicos em escritórios e ambientes de trabalho.\n",
    "\n",
    "Hidráulico:\n",
    "\n",
    "Quantidade: 1.387 itens Percentual: 14,10% A categoria \"Hidráulico\" abrange aproximadamente 14,10% do total de itens. Itens hidráulicos são cruciais para sistemas e processos relacionados à distribuição e controle de água e fluidos.\n",
    "\n",
    "Essa análise permite uma compreensão clara da composição do inventário, destacando a importância de cada categoria na estrutura geral do estoque. Isso pode ser valioso para a tomada de decisões estratégicas, otimização do estoque e atendimento às necessidades específicas do negócio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoria: Embalagem, Quantidade: 3416, Percentual: 34.72%\n",
      "Categoria: Epi, Quantidade: 2673, Percentual: 27.16%\n",
      "Categoria: Papelaria, Quantidade: 2364, Percentual: 24.02%\n",
      "Categoria: Hidráulico, Quantidade: 1387, Percentual: 14.10%\n"
     ]
    }
   ],
   "source": [
    "#Listagem das categorias e as quantidades com percentual\n",
    "contagem_categoria = df['Categoria'].value_counts()\n",
    "total = len(df)  #Total de itens\n",
    "\n",
    "for categoria, quantidade in contagem_categoria.items():\n",
    "    porcentagem = (quantidade / total) * 100\n",
    "    print(f\"Categoria: {categoria}, Quantidade: {quantidade}, Percentual: {porcentagem:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Início do Pré Processamento**\n",
    "\n",
    "Criação de funções de pré processamento com as libs SpellChecker, NLTK e unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfManipulated = df.copy() #Copia do dataset original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o total de itens\n",
    "total = len(df)\n",
    "\n",
    "# Calcular o percentual e a quantidade para cada categoria\n",
    "porcentagens = []\n",
    "quantidades = []\n",
    "categorias = []\n",
    "labels = []  # Lista para armazenar os rótulos\n",
    "for categoria, grupo in df.groupby('Categoria'):\n",
    "    quantidade = len(grupo)\n",
    "    porcentagem = (quantidade / total) * 100\n",
    "    porcentagens.append(porcentagem)\n",
    "    quantidades.append(quantidade)\n",
    "    categorias.append(categoria)\n",
    "    labels.append(f'{categoria}<br>({quantidade} itens)')\n",
    "\n",
    "# Cores para cada categoria\n",
    "cores = ['blue', 'orange', 'green', 'red']\n",
    "\n",
    "# Ajustar os raios para 80% da área\n",
    "raio_externo = 0.4  # Raio externo para 80% da área\n",
    "raio_interno = raio_externo - 0.02  # Raio interno para criar o \"anel\"\n",
    "\n",
    "# Criar o gráfico de pizza\n",
    "fig = go.Figure(data=[go.Pie(labels=labels,\n",
    "                             values=porcentagens,\n",
    "                             textinfo='label+percent',  # Exibir rótulos e percentuais\n",
    "                             hole=raio_interno,  # Raio interno\n",
    "                             marker=dict(colors=cores))])\n",
    "\n",
    "# Adicionar título e ajustar layout\n",
    "fig.update_layout(title_text=f'Distribuição dos Itens por Categorias - Total: {total}',\n",
    "                  margin=dict(t=50, b=50, l=50, r=50))  # Margens para remover espaços desnecessários\n",
    "\n",
    "# Mostrar o gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker(language='pt') #Instanciando o objeto SpellChecker\n",
    "\n",
    "#A função abaixo utilizando a biblioteca SpellChecker corrige as palavras que estão escritas erradas.\n",
    "def pyspellchecker(text):\n",
    "    word_list = word_tokenize(text, language = 'portuguese')\n",
    "    word_list_corrected = []\n",
    "    for word in word_list:\n",
    "        if word in spell.unknown(word_list) and len(word) > 3:\n",
    "            word_corrected = spell.correction(word)\n",
    "            if word_corrected == None:\n",
    "                word_list_corrected.append(word)\n",
    "            else:\n",
    "                word_list_corrected.append(word_corrected)\n",
    "        else:\n",
    "            word_list_corrected.append(word)\n",
    "    text_corrected = \" \".join(word_list_corrected)\n",
    "    return text_corrected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A função abaixo utilizando a biblioteca unicodedata, remove os acentos das palavras.\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A função abaixo utiliza a biblioteca NLTK para remover as stopwords, pontuações, caracteres especiais, palavras com menos de 2 caracteres e aplica a lemmatization.\n",
    "def limpar_texto(text):\n",
    "   text = str(text)\n",
    "\n",
    "    # Remover caracteres non-ascii\n",
    "   text = ''.join(caracter for caracter in text if ord(caracter) < 128)\n",
    "\n",
    "    # Convertendo para lower case\n",
    "   text = text.lower()\n",
    "\n",
    "    # Removendo pontuação por expressão regular\n",
    "   regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n",
    "   text = regex.sub(' ', str(text))\n",
    "\n",
    "   text = re.sub('[^A-Za-z ]+', '', text)\n",
    "\n",
    "    # Carregando stopwords em português\n",
    "   portuguese_stops = set(stopwords.words('portuguese'))\n",
    "\n",
    "    # Removendo stopwords em português\n",
    "    # Mantendo somente palavras que não são consideradas stopwords\n",
    "   text = ' '.join(palavra for palavra in text.split() if palavra not in portuguese_stops)\n",
    "\n",
    "    # Remover palavras com menos de 2 caracteres\n",
    "   words = text.split()\n",
    "   text = [word for word in words if len(word) > 2 or word == ' ']\n",
    "   text = ' '.join(text)\n",
    "\n",
    "    # Corrigindo erros gramaticais\n",
    "   text = pyspellchecker(text)\n",
    "\n",
    "    # Criando a estrutura baseada em uma wordnet para lemmatization\n",
    "   wordnet_lemmatizer = WordNetLemmatizer()\n",
    "   \n",
    "   # Aplicando Lemmatization\n",
    "   text = ' '.join(wordnet_lemmatizer.lemmatize(palavra) for palavra in text.split())\n",
    "\n",
    "    # Remover palavras com menos de 2 caracteres residuais\n",
    "   words = text.split()\n",
    "   text = [word for word in words if len(word) > 2 or word == ' ']\n",
    "   text = ' '.join(text)\n",
    "\n",
    "   text = strip_accents(text)\n",
    "\n",
    "   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Tratanto texto...')\n",
    "dfManipulated['descricao'] = dfManipulated['descricao'].map(limpar_texto)\n",
    "print('Texto tratado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exporta o dataframe pre processado para formato .csv e em seguida atribui-se a variavel df esse novo .csv pre processado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporta o DataFrame pre processado para um arquivo .csv\n",
    "dfManipulated.to_csv('data/dfPreProcessado.csv', index=False)\n",
    "df = pd.read_csv(r'data/dfPreProcessado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a função a seguir é responsável por exibir a descrição de um produto aleatório ou filtrado, caso deseje passar no parametro.\n",
    "def exibir_descricao(filtro=None):\n",
    "    if filtro:\n",
    "        descricao_filtrada = df[df['descricao'].str.contains(filtro, case=False)]\n",
    "        descricao_list = descricao_filtrada['descricao'].tolist()\n",
    "    else:\n",
    "        descricao_list = df['descricao'].sample(10).tolist()  # Seleciona 10 itens aleatórios\n",
    "\n",
    "    # Criar uma tabela com alinhamento à esquerda\n",
    "    tabela = PrettyTable()\n",
    "    tabela.field_names = ['Descrição']\n",
    "    tabela.align['Descrição'] = 'l'  # Alinha à esquerda\n",
    "\n",
    "    # Adicionar as descrições à tabela\n",
    "    for descricao in descricao_list:\n",
    "        tabela.add_row([descricao])\n",
    "\n",
    "    # Exibir a tabela\n",
    "    print(tabela)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identificação e criação de csv's para Frases repetidas, frases que aparecem em mais de uma categoria e palavras iguais.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRIAÇÃO DE CSV COM FRASES REPETIDAS\n",
    "\n",
    "# Limpeza dos espaços no início e final das frases\n",
    "df['descricao'] = df['descricao'].str.strip()\n",
    "\n",
    "# Identificação de frases repetidas\n",
    "frases_repetidas = df[df.duplicated('descricao', keep=False)]\n",
    "\n",
    "# Contagem das frases repetidas\n",
    "contagem_frases_repetidas = frases_repetidas['descricao'].value_counts()\n",
    "\n",
    "# Criar um DataFrame com a quantidade e descrição repetida\n",
    "df_contagem = pd.DataFrame({'descricao_repetida': contagem_frases_repetidas.index,\n",
    "                            'quantidade': contagem_frases_repetidas.values})\n",
    "\n",
    "# Adicionar coluna com frase e quantidade\n",
    "df_contagem['frase_quantidade'] = df_contagem.apply(lambda row: f\"{row['descricao_repetida']}, Quantidade: {row['quantidade']}\", axis=1)\n",
    "\n",
    "# Exportar para CSV\n",
    "df_contagem.to_csv('data/frases_repetidas.csv', columns=['frase_quantidade'], index=False)\n",
    "print(\"Csv Frases repetidas e suas contagens criado na pasta data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERIFICANDO FRASES QUE APARECEM EM MAIS DE UMA CATEGORIA\n",
    "# Limpeza dos espaços no início e final das frases\n",
    "df['descricao'] = df['descricao'].str.strip()\n",
    "\n",
    "# Contar quantas vezes cada frase aparece para diferentes categorias\n",
    "contagem_frases_por_categoria = df.groupby(['descricao', 'Categoria']).size().reset_index(name='contagem')\n",
    "\n",
    "# Filtrar frases que aparecem em mais de uma categoria\n",
    "frases_em_mais_de_uma_categoria = contagem_frases_por_categoria[contagem_frases_por_categoria.duplicated('descricao', keep=False)]\n",
    "\n",
    "# Exibir as frases que aparecem em mais de uma categoria\n",
    "print(\"Frases que aparecem em mais de uma categoria:\")\n",
    "print(frases_em_mais_de_uma_categoria)\n",
    "print(\"Criado o CSV frases que aparecem em mais de uma categoria\")\n",
    "frases_em_mais_de_uma_categoria.to_csv('data/frases_em_mais_de_uma_categoria.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#essa função retorna a quantidade de ocorrências de 'GALV' (maiúscula ou minúscula) com a possibilidade de um ponto após a última letra\n",
    "def contar_ocorrencias_galv(texto):\n",
    "    padrao = re.compile(r'(?i)GALV\\.?')\n",
    "    return len(padrao.findall(texto))\n",
    "\n",
    "# Aplicar a função a cada linha da coluna 'descricao'\n",
    "df['contagem_galv'] = df['descricao'].apply(contar_ocorrencias_galv)\n",
    "\n",
    "# Calcular o total de ocorrências de 'GALV' (maiúscula ou minúscula) com a possibilidade de um ponto após a última letra\n",
    "total_ocorrencias_galv = sum(df['contagem_galv'])\n",
    "\n",
    "# Exibir o total de ocorrências\n",
    "print('Total de ocorrências de \"GALV\" (com ou sem ponto após a última letra):', total_ocorrencias_galv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para identificar palavras iguais em sequência e contar sua ocorrência\n",
    "def identificar_palavras_iguais(texto):\n",
    "    padrao = r'\\b(\\w+)\\s+\\1\\b'\n",
    "    palavras_iguais = re.findall(padrao, texto, flags=re.IGNORECASE)\n",
    "    quantidade = len(palavras_iguais)\n",
    "    \n",
    "    return palavras_iguais, quantidade\n",
    "\n",
    "# Identificar palavras iguais em sequência e contar a quantidade para cada descrição\n",
    "df[['palavras_iguais', 'quantidade']] = df['descricao'].apply(lambda x: pd.Series(identificar_palavras_iguais(x), index=['palavras_iguais', 'quantidade']))\n",
    "\n",
    "# Filtrar e exibir apenas as linhas com palavras duplicadas\n",
    "df_com_palavras_duplicadas = df[df['quantidade'] > 0]\n",
    "\n",
    "# Criar um DataFrame para o CSV contendo as palavras duplicadas e a frase correspondente\n",
    "df_csv = pd.DataFrame({'Palavra_Duplicada': df_com_palavras_duplicadas['palavras_iguais'].apply(lambda x: x[0]),\n",
    "                       'Frase': df_com_palavras_duplicadas['descricao']})\n",
    "\n",
    "# Salvar o DataFrame em um arquivo CSV\n",
    "df_csv.to_csv('data/palavras_duplicadas.csv', index=False)\n",
    "\n",
    "print(\"Arquivo CSV 'palavras_duplicadas.csv' criado com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representação gráfica das palavras duplicadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o arquivo CSV em um DataFrame\n",
    "df = pd.read_csv('data/palavras_duplicadas.csv')\n",
    "\n",
    "# Calcular a quantidade de ocorrências para cada palavra duplicada\n",
    "contagem_palavras = df['Palavra_Duplicada'].value_counts()\n",
    "\n",
    "# Filtrar para palavras com quantidade maior que 50 e menor ou igual a 50\n",
    "maior_que_50 = contagem_palavras[contagem_palavras > 50]\n",
    "menor_ou_igual_a_50 = contagem_palavras[contagem_palavras <= 50]\n",
    "\n",
    "# Criar o gráfico para quantidade maior que 50\n",
    "fig_maior_que_50 = go.Figure(go.Bar(\n",
    "    y=maior_que_50.index,\n",
    "    x=maior_que_50.values,\n",
    "    orientation='h',\n",
    "    text=maior_que_50.values,\n",
    "    textposition='auto',\n",
    "    hoverinfo='text',\n",
    "    marker=dict(color='royalblue')\n",
    "))\n",
    "\n",
    "fig_maior_que_50.update_layout(\n",
    "    title='Quantidade de Palavras Duplicadas (Quantidade > 50)',\n",
    "    xaxis_title='Quantidade',\n",
    "    yaxis_title='Palavra Duplicada',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Criar o gráfico para quantidade menor ou igual a 50\n",
    "fig_menor_ou_igual_a_50 = go.Figure(go.Bar(\n",
    "    y=menor_ou_igual_a_50.index,\n",
    "    x=menor_ou_igual_a_50.values,\n",
    "    orientation='h',\n",
    "    text=menor_ou_igual_a_50.values,\n",
    "    textposition='auto',\n",
    "    hoverinfo='text',\n",
    "    marker=dict(color='purple')\n",
    "))\n",
    "\n",
    "fig_menor_ou_igual_a_50.update_layout(\n",
    "    title='Quantidade de Palavras Duplicadas (Quantidade <= 50)',\n",
    "    xaxis_title='Quantidade',\n",
    "    yaxis_title='Palavra Duplicada',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Exibir os gráficos\n",
    "fig_maior_que_50.show()\n",
    "fig_menor_ou_igual_a_50.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remoção de palavras iguais em sequencia. Ex: Cano Hidraulico Hidraulico**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover plavras iguias em sequencia exemplo cor cor\n",
    "def remover_palavras_iguais_em_sequencia(texto):\n",
    "    padrao = r'\\b(\\w+)\\b\\s+\\1\\b'\n",
    "    texto_sem_repeticao = re.sub(padrao, r'\\1', texto, flags=re.IGNORECASE)\n",
    "    return texto_sem_repeticao\n",
    "\n",
    "df = pd.read_csv(r'data/dfPreProcessado.csv')\n",
    "\n",
    "# Aplicar a função para remover palavras iguais em sequência na coluna \"descricao\"\n",
    "df['descricao'] = df['descricao'].apply(remover_palavras_iguais_em_sequencia)\n",
    "print(\"Palavras iguais em sequência removidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para identificar palavras-chave em uma descrição manual informe a palavra no array palavras_chave \n",
    "def identificar_palavras_chave(descricao):\n",
    "    palavras_chave = ['filial', 'vender', 'fort', 'miguel','uso']\n",
    "    presentes = {palavra: descricao.lower().count(palavra) for palavra in palavras_chave}\n",
    "    return presentes\n",
    "\n",
    "# Calcular a quantidade de cada palavra-chave encontrada\n",
    "df['palavras_chave_quantidade'] = df['descricao'].apply(identificar_palavras_chave)\n",
    "\n",
    "# Calcular a soma da quantidade de cada palavra-chave\n",
    "soma_palavras_chave = df['palavras_chave_quantidade'].apply(pd.Series).sum().sort_values()\n",
    "\n",
    "# Criar um gráfico de barras verticais com plotly\n",
    "fig = go.Figure(go.Bar(\n",
    "    y=soma_palavras_chave.index,\n",
    "    x=soma_palavras_chave.values,\n",
    "    text=soma_palavras_chave.values,\n",
    "    textposition='auto',\n",
    "    marker_color='royalblue',\n",
    "    orientation='h',\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Quantidade de cada Palavra-chave',\n",
    "    xaxis_title='Palavra-chave',\n",
    "    yaxis_title='Quantidade',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Exibir o gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'data/dfPreProcessado.csv')\n",
    "\n",
    "# Função para contar palavras de até 3 caracteres\n",
    "def contar_palavras_ate_3_caracteres(texto):\n",
    "    palavras = texto.lower().split()\n",
    "    palavras_ate_3 = [palavra for palavra in palavras if len(palavra) == 3]\n",
    "    return Counter(palavras_ate_3)\n",
    "\n",
    "\n",
    "# Calcular a contagem de palavras de até 3 caracteres em todas as descrições\n",
    "contagem_palavras = Counter()\n",
    "for descricao in df['descricao']:\n",
    "    contagem_palavras += contar_palavras_ate_3_caracteres(descricao)\n",
    "\n",
    "# Filtrar palavras com quantidade maior que 500 ou maior que 100\n",
    "palavras_maior_500 = {palavra: contagem_palavras[palavra] for palavra in contagem_palavras if contagem_palavras[palavra] > 500}\n",
    "palavras_maior_100 = {palavra: contagem_palavras[palavra] for palavra in contagem_palavras if 100 < contagem_palavras[palavra] <= 500}\n",
    "palavras_menor_100 = {palavra: contagem_palavras[palavra] for palavra in contagem_palavras if contagem_palavras[palavra] <= 100}\n",
    "\n",
    "# Criar um gráfico de barras para quantidade maior que 500\n",
    "fig_maior_500 = go.Figure(go.Bar(\n",
    "    y=list(palavras_maior_500.keys()),\n",
    "    x=list(palavras_maior_500.values()),\n",
    "    text=list(palavras_maior_500.values()),\n",
    "    textposition='auto',\n",
    "    marker_color='royalblue',\n",
    "    orientation='h',\n",
    "))\n",
    "\n",
    "fig_maior_500.update_layout(\n",
    "    title='Quantidade de Palavras com mais de 500 ocorrências',\n",
    "    xaxis_title='Palavra',\n",
    "    yaxis_title='Quantidade',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Criar um gráfico de barras para quantidade entre 100 e 500\n",
    "fig_entre_100_e_500 = go.Figure(go.Bar(\n",
    "    y=list(palavras_maior_100.keys()),\n",
    "    x=list(palavras_maior_100.values()),\n",
    "    text=list(palavras_maior_100.values()),\n",
    "    textposition='auto',\n",
    "    marker_color='purple',\n",
    "    orientation='h',\n",
    "))\n",
    "\n",
    "fig_entre_100_e_500.update_layout(\n",
    "    title='Quantidade de Palavras com mais de 100 e até 500 ocorrências',\n",
    "    xaxis_title='Palavra',\n",
    "    yaxis_title='Quantidade',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Criar um gráfico de barras para quantidade menor que 100\n",
    "fig_menor_100 = go.Figure(go.Bar(\n",
    "    y=list(palavras_menor_100.keys()),\n",
    "    x=list(palavras_menor_100.values()),\n",
    "    text=list(palavras_menor_100.values()),\n",
    "    textposition='auto',\n",
    "    marker_color='green',\n",
    "    orientation='h',\n",
    "))\n",
    "\n",
    "fig_menor_100.update_layout(\n",
    "    title='Quantidade de Palavras com até 100 ocorrências',\n",
    "    xaxis_title='Palavra',\n",
    "    yaxis_title='Quantidade',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Exibir os gráficos\n",
    "fig_maior_500.show()\n",
    "fig_entre_100_e_500.show()\n",
    "fig_menor_100.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criado esta função para remover as palavras desejada a atualizado o dataframe\n",
    "# Para remover uma palavra adicione as palavras que deseja remover no array\n",
    "stop_words_personalizadas = ['ete', 'ftc', 'xxm','ref'\n",
    "                             'saw', 'shp','uso',\n",
    "                             'filial','preto','preta','vermelho','vermelha', 'azul', 'verde', 'amarelo', 'laranja']\n",
    "def remover_stop_words(texto):\n",
    "    # Padroniza a lista de stop words para garantir que a comparação seja insensível a maiúsculas e minúsculas\n",
    "    stop_words = [re.escape(word.lower()) for word in stop_words_personalizadas]\n",
    "\n",
    "    # Cria uma expressão regular para encontrar as stop words\n",
    "    padrao = r'\\b(?:' + '|'.join(stop_words) + r')\\b'\n",
    "    \n",
    "    # Remove as stop words do texto\n",
    "    texto_sem_stop_words = re.sub(padrao, '', texto, flags=re.IGNORECASE)\n",
    "    return texto_sem_stop_words\n",
    "\n",
    "\n",
    "df = pd.read_csv(r'data/dfPreProcessado.csv')\n",
    "df['descricao'] = df['descricao'].apply(remover_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função criada para fazer replace em massa das palavras\n",
    "def replace_em_massa_dataframe(df, coluna, vetor_valores_substituir, vetor_novos_valores):\n",
    "    \n",
    "    # Garante que os tamanhos dos vetores são iguais\n",
    "    if len(vetor_valores_substituir) != len(vetor_novos_valores):\n",
    "        raise ValueError(\"Os vetores de substituição devem ter o mesmo tamanho\")\n",
    "\n",
    "    # Cria um dicionário de substituições\n",
    "    substituicoes = dict(zip(vetor_valores_substituir, vetor_novos_valores))\n",
    "\n",
    "    # Aplica as substituições na coluna \"descricao\"\n",
    "    df[coluna] = df[coluna].replace(substituicoes, regex=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Passe as palavras neste array \n",
    "vetor_valores_substituir = [\"canetass\",\n",
    "                            \"luvas\",\n",
    "                            \"filial\",\n",
    "                            \"GALV\",\n",
    "                          \n",
    "                            ]\n",
    "vetor_novos_valores = [\"caneta\", \n",
    "                       \"luva\",\n",
    "                       \"\",\n",
    "                       \"galvanizada\",\n",
    "                       \n",
    "                     ]\n",
    "\n",
    "df = replace_em_massa_dataframe(df, 'descricao', vetor_valores_substituir, vetor_novos_valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copia = df\n",
    "df_copia['descricao'] = df_copia['descricao'].str.strip()\n",
    "\n",
    "# Remover frases duplicadas\n",
    "df_sem_duplicatas = df_copia.drop_duplicates(subset=['descricao'])\n",
    "\n",
    "#Exporta o csv resultante\n",
    "df_sem_duplicatas.to_csv('data/df_sem_duplicatas.csv', index=False)\n",
    "\n",
    "# Exibir DataFrame sem frases duplicadas\n",
    "print(\"DataFrame sem frases duplicadas:\")\n",
    "print(df_sem_duplicatas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
