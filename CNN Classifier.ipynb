{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importando pacotes e bibliotecas que serão úteis para o CNN classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Conv1D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carregando a base de dados a ser utilizada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data/df_sem_duplicatas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set-Up do LabelEncoder model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['categoria'] = label_encoder.fit_transform(df['categoria'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separando em treino e teste**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['descricao'].values\n",
    "y = df['categoria'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length =  max(len(text) for text in df['descricao'])  \n",
    "\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=len(tokenizer.word_index) + 1) \n",
    "X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "initial_output_dim =1\n",
    "initial_cov1d_filters =2\n",
    "initial_dense_units = 1\n",
    "increment = True\n",
    "i = 1\n",
    "epochs =50\n",
    "monitor_metric = 'val_loss'\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=monitor_metric, mode='min', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representação grafica do teste / treino**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(X_train)\n",
    "num_test = len(X_test)\n",
    "\n",
    "# Crie um gráfico de barras para mostrar a divisão\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.bar(['Validação', 'Teste'], [num_train, num_test], color=['blue', 'green'])\n",
    "plt.xlabel('Conjunto de Dados')\n",
    "plt.ylabel('Quantidade de Exemplos')\n",
    "plt.title('Divisão entre Validação e Teste')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo Inicial de treinamento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esse loop é para aumentar a densidade da camada densa até que o early stopping seja ativado\n",
    "densidade = 1  \n",
    "while True: \n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(densidade, activation='relu', input_shape=(X_train_tfidf.shape[1],)),  \n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(X_train_tfidf, y_train, epochs=epochs, validation_data=(X_test_tfidf, y_test), verbose=0, callbacks=[early_stopping])\n",
    "    if early_stopping.stopped_epoch > 0:\n",
    "        break\n",
    "    densidade += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificação grafica da precisao e da perda do modelo inicial\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Treino')\n",
    "plt.plot(val_acc, label='Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisão')\n",
    "plt.legend()\n",
    "plt.title('Precisão - Treino e Validação')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Treino')\n",
    "plt.plot(val_loss, label='Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "plt.title('Perda - Treino e Validação')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relatorio da classificação\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred_classes)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_original, y_pred_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resumo do modelo\n",
    "print(f\"Densidade: {densidade:.4f}\")\n",
    "print(f\"Numero de classes: {num_classes:.4f}\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotagem da matriz de confusão\n",
    "conf_matrix = confusion_matrix(y_test_original,y_pred_original)\n",
    "labels = np.unique(y_pred_original)\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, labels)\n",
    "plt.yticks(tick_marks, labels)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        plt.text(j, i, str(conf_matrix[i, j]), horizontalalignment='center', verticalalignment='center')\n",
    "plt.ylabel('Rótulo Verdadeiro')\n",
    "plt.xlabel('Rótulo Predito')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Otimização do Modelo Inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esse loop é para aumentar a densidade da camada densa até que o early stopping seja ativado\n",
    "densidade = 1  \n",
    "while True:\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(densidade, activation='relu', input_shape=(X_train_tfidf.shape[1],)),  \n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.001)  \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    history = model.fit(X_train_tfidf, y_train, epochs=epochs, validation_data=(X_test_tfidf, y_test), verbose=0, callbacks=[early_stopping])\n",
    "    if early_stopping.stopped_epoch > 0:\n",
    "        break\n",
    "    densidade += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificação grafica da precisao e da perda \n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Treino')\n",
    "plt.plot(val_acc, label='Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisão')\n",
    "plt.legend()\n",
    "plt.title('Precisão - Treino e Validação')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Treino')\n",
    "plt.plot(val_loss, label='Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "plt.title('Perda - Treino e Validação')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relatorio da classificação\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred_classes)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_original, y_pred_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resumo do modelo\n",
    "print(f\"Densidade: {densidade:.4f}\")\n",
    "print(f\"Numero de classes: {num_classes:.4f}\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotagem da matriz de confusão\n",
    "conf_matrix = confusion_matrix(y_test_original,y_pred_original)\n",
    "labels = np.unique(y_pred_original)\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, labels)\n",
    "plt.yticks(tick_marks, labels)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        plt.text(j, i, str(conf_matrix[i, j]), horizontalalignment='center', verticalalignment='center')\n",
    "plt.ylabel('Rótulo Verdadeiro')\n",
    "plt.xlabel('Rótulo Predito')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo com Conv1D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esse loop no modelo Conv1D é para aumentar o numero de filtros até que o early stopping seja ativado\n",
    "densidade = 1 \n",
    "cov1d_filters = 1 \n",
    "while True:\n",
    "    model = tf.keras.Sequential([\n",
    "        Conv1D(filters=cov1d_filters, kernel_size=3, activation='relu', input_shape=(X_train_tfidf.shape[1], 1)),  \n",
    "        Flatten(),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    X_train_tfidf_reshaped = X_train_tfidf.reshape(X_train_tfidf.shape[0], X_train_tfidf.shape[1], 1)\n",
    "    X_test_tfidf_reshaped = X_test_tfidf.reshape(X_test_tfidf.shape[0], X_test_tfidf.shape[1], 1)\n",
    "    history = model.fit(X_train_tfidf_reshaped, y_train, epochs=epochs, validation_data=(X_test_tfidf_reshaped, y_test), verbose=0, callbacks=[early_stopping])\n",
    "    if early_stopping.stopped_epoch > 0:\n",
    "        break\n",
    "    cov1d_filters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificação grafica da precisao e da perda \n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Treino')\n",
    "plt.plot(val_acc, label='Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisão')\n",
    "plt.legend()\n",
    "plt.title('Precisão - Treino e Validação')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Treino')\n",
    "plt.plot(val_loss, label='Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "plt.title('Perda - Treino e Validação')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotagem da Matriz de Confusão\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred_classes)\n",
    "conf_matrix = confusion_matrix(y_test_original,y_pred_original)\n",
    "labels = np.unique(y_pred_original)\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, labels)\n",
    "plt.yticks(tick_marks, labels)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        plt.text(j, i, str(conf_matrix[i, j]), horizontalalignment='center', verticalalignment='center')\n",
    "plt.ylabel('Rótulo Verdadeiro')\n",
    "plt.xlabel('Rótulo Predito')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resumo da classificação\n",
    "print(f\"Densidade: {densidade:.4f}\")\n",
    "print(f\"Numero de classes: {num_classes:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_original, y_pred_original))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Otimizando modelo com Conv1D - Adicionando Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esse loop incrementa o numero de filtros até que o early stopping seja ativado\n",
    "while increment:\n",
    "    output_dim = initial_output_dim + 1 \n",
    "    cov1d_filters = initial_cov1d_filters + 1\n",
    "    model = tf.keras.Sequential([\n",
    "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=output_dim, input_length=max_length),\n",
    "        Conv1D(filters=cov1d_filters, kernel_size=1, activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(num_classes, activation='softmax')  \n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train_padded, y_train, epochs=epochs, validation_data=(X_test_padded, y_test), verbose=0, callbacks=[early_stopping])\n",
    "    if early_stopping.stopped_epoch > 0:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificação grafica da precisao e da perda \n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Treino')\n",
    "plt.plot(val_acc, label='Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisão')\n",
    "plt.legend()\n",
    "plt.title('Precisão - Treino e Validação')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Treino')\n",
    "plt.plot(val_loss, label='Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "plt.title('Perda - Treino e Validação')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicionando optimizer RMSprop\n",
    "while increment:\n",
    "    output_dim = initial_output_dim + 1 \n",
    "    cov1d_filters = initial_cov1d_filters + 1\n",
    "    model = tf.keras.Sequential([\n",
    "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=output_dim, input_length=max_length),\n",
    "        Conv1D(filters=cov1d_filters, kernel_size=1, activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(num_classes, activation='softmax')  \n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train_padded, y_train, epochs=epochs, validation_data=(X_test_padded, y_test), verbose=0, callbacks=[early_stopping])\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    if early_stopping.stopped_epoch > 0:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificação grafica da precisao e da perda\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Treino')\n",
    "plt.plot(val_acc, label='Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisão')\n",
    "plt.legend()\n",
    "plt.title('Precisão - Treino e Validação')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Treino')\n",
    "plt.plot(val_loss, label='Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "plt.title('Perda - Treino e Validação')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação do modelo e obtenção as previsões\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotagem da matriz de confusão\n",
    "conf_matrix = confusion_matrix(y_test_original,y_pred_original)\n",
    "labels = np.unique(y_pred_original)\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, labels)\n",
    "plt.yticks(tick_marks, labels)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        plt.text(j, i, str(conf_matrix[i, j]), horizontalalignment='center', verticalalignment='center')\n",
    "plt.ylabel('Rótulo Verdadeiro')\n",
    "plt.xlabel('Rótulo Predito')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relatório da classificação\n",
    "print(f\"Output dim: {output_dim:.4f}\")\n",
    "print(f\"cov1d_filters: {cov1d_filters:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_original, y_pred_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resumo do modelo\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
